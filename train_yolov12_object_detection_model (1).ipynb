{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NyQxnvGsjH7D"
      },
      "source": [
        "[![Roboflow Notebooks](https://media.roboflow.com/notebooks/template/bannertest2-2.png?ik-sdk-version=javascript-1.4.3&updatedAt=1672932710194)](https://github.com/roboflow/notebooks)\n",
        "\n",
        "# How to Train YOLOv12 Object Detection on a Custom Dataset\n",
        "\n",
        "---\n",
        "\n",
        "[![Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/roboflow-ai/notebooks/blob/main/notebooks/train-yolov12-object-detection-model.ipynb)\n",
        "[![arXiv](https://img.shields.io/badge/arXiv-2502.12524-b31b1b.svg)](https://arxiv.org/abs/2502.12524)\n",
        "[![Roboflow](https://raw.githubusercontent.com/roboflow-ai/notebooks/main/assets/badges/roboflow-blogpost.svg)](https://blog.roboflow.com/train-yolov12-model)\n",
        "[![GitHub](https://badges.aleen42.com/src/github.svg)](https://github.com/sunsmarterjie/yolov12)\n",
        "\n",
        "[YOLOv12](https://github.com/sunsmarterjie/yolov12) is a newly proposed attention-centric variant of the YOLO family that focuses on incorporating efficient attention mechanisms into the backbone while preserving real-time performance. Instead of relying heavily on CNN-based architectures like its predecessors, YOLOv12 introduces a simple yet powerful “area attention” module, which strategically partitions the feature map to reduce the quadratic complexity of full self-attention. It also adopts residual efficient layer aggregation networks (R-ELAN) to enhance feature aggregation and training stability, especially for larger models. These innovations, together with refinements such as scaled residual connections and a reduced MLP ratio, enable YOLOv12 to harness the benefits of attention (e.g., better global context modeling) without sacrificing speed.\n",
        "\n",
        "![yolov12-area-attention](https://media.roboflow.com/notebooks/examples/yolov12-area-attention.png)\n",
        "\n",
        "Compared to prior YOLO iterations (e.g., YOLOv10, YOLOv11, and YOLOv8), YOLOv12 achieves higher detection accuracy with competitive or faster inference times across all model scales. Its five sizes—N, S, M, L, and X—range from 2.6M to 59.1M parameters, striking a strong accuracy–speed balance. For instance, the smallest YOLOv12-N surpasses other “nano” models by over 1% mAP with latency around 1.6 ms on a T4 GPU, and the largest YOLOv12-X achieves 55.2% mAP, comfortably outscoring comparable real-time detectors such as RT-DETR and YOLOv11-X . By matching or exceeding state-of-the-art accuracy while remaining fast, YOLOv12 represents a notable step forward for attention-based real-time object detection.\n",
        "\n",
        "![yolov12-metrics](https://storage.googleapis.com/com-roboflow-marketing/notebooks/examples/yolov12-metrics.png)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DaGgQiprFPvO"
      },
      "source": [
        "## Environment setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kFKPGsPfOdsk"
      },
      "source": [
        "### Configure your API keys\n",
        "\n",
        "To fine-tune YOLOv12, you need to provide your Roboflow API key. Follow these steps:\n",
        "\n",
        "- Go to your [`Roboflow Settings`](https://app.roboflow.com/settings/api) page. Click `Copy`. This will place your private key in the clipboard.\n",
        "- In Colab, go to the left pane and click on `Secrets` (🔑). Store Roboflow API Key under the name `ROBOFLOW_API_KEY`."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "OFc4HsSbOzKp"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "\n",
        "# Directly set your Roboflow API key\n",
        "os.environ[\"ROBOFLOW_API_KEY\"] = \"wGYGwTjglhST7mybt9qF\"\n",
        "\n",
        "### Check GPU availability\n",
        "\n",
        "**NOTE:** **YOLOv12 leverages FlashAttention to speed up attention-based computations, but this feature requires an Nvidia GPU built on the Ampere architecture or newer—for example, GPUs like the RTX 3090, RTX 3080, or even the Nvidia L4 meet this requirement.**\n",
        "\n",
        "Let's make sure that we have access to GPU. We can use `nvidia-smi` command to do that. In case of any problems navigate to `Edit` -> `Notebook settings` -> `Hardware accelerator`, set it to `GPU`, and then click `Save`.\n",
        "!nvidia-smi\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)\n",
        "### Install dependencies\n",
        "\n",
        "**NOTE:** C\n",
        "!pip install -q git+https://github.com/sunsmarterjie/yolov12.git roboflow supervision flash-attn\n",
        "!pip install -q git+https://github.com/sunsmarterjie/yolov12.git roboflow supervision flash-attnurrently, YOLOv12 does not have its own PyPI package, so we install it directly from GitHub while also adding roboflow (to conveniently pull datasets from the Roboflow Universe), supervision (to visualize inference results and benchmark the model’s performance), and flash-attn (to accelerate attention-based computations via optimized CUDA kernels).\n",
        "import os\n",
        "### Download example data\n",
        "\n",
        "Let's download an image we can use for YOLOv12 inference. Feel free to drag and drop your own images into the Files tab on the left-hand side of Google Colab, then reference their filenames in your code for a custom inference demo.\n",
        "### Downloa\n",
        "## Download\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Initializ\n",
        "import os\n",
        "print(dataset.location)\n",
        "\n",
        "**NOTE:** We need to make a few changes to our downloaded dataset so it will work with YOLOv12. Run the following bash commands to prepare your dataset for training by updating the relative paths in the `data.yaml` file, ensuring it correctly points to the subdirectories for your dataset's `train`, `test`, and `valid` subsets.\n",
        "\n",
        "!cat {dataset.location}/data.yaml\n",
        "!cat {dataset.location}/data.yamlimport os\n",
        "print(dataset.location)\n",
        "e Roboflow\n",
        "rf = Roboflow(api_key=\"wGYGwTjglhST7mybt9qF\")\n",
        "\n",
        "# Load your workspace and project\n",
        "project = rf.workspace(\"da-mec7y\").project(\"cataract-finder-nups1\")\n",
        "\n",
        "# Get the version you want (e.g., version 1) and specify format as 'yolov12'\n",
        "\n",
        "## Fine-tune YOLOv12 model\n",
        "\n",
        "We are now ready to fine-tune our YOLOv12 model. In the code below, we initialize the model using a starting checkpoint—here, we use `yolov12s.yaml`, but you can replace it with any other model (e.g., `yolov12n.pt`, `yolov12m.pt`, `yolov12l.pt`, or `yolov12x.pt`) based on your preference. We set the training to run for 100 epochs in this example; however, you should adjust the number of epochs along with other hyperparameters such as batch size, image size, and augmentation settings (scale, mosaic, mixup, and copy-paste) based on your hardware capabilities and dataset size.\n",
        "\n",
        "**Note:** **Note that after training, you might encounter a `TypeError: argument of type 'PosixPath' is not iterable error` — this is a known issue, but your model weights will still be saved, so you can safely proceed to running inference.**\n",
        "## Fine-tune YOLOv12 model\n",
        "\n",
        "We are now ready to fine-tune our YOLOv12 model. In the code below, we initialize the model using a starting checkpoint—here, we use `yolov12s.yaml`, but you can replace it with any other model (e.g., `yolov12n.pt`, `yolov12m.pt`, `yolov12l.pt`, or `yolov12x.pt`) based on your preference. We set the training to run for 100 epochs in this example; however, you should adjust the number of epochs along with other hyperparameters such as batch size, image size, and augmentation settings (scale, mosaic, mixup, and copy-paste) based on your hardware capabilities and dataset size.\n",
        "\n",
        "**Note:** **Note that after training, you might encounter a `TypeError: argument of type 'PosixPath' is not iterable error` — this is a known issue, but your model weights will still be saved, so you can safely proceed to running inference.**dataset = project.version(1).download(\"yolov12\")\n",
        "\n",
        "from ultralytics import YOLO\n",
        "\n",
        "model = YOLO('yolov12s.yaml')\n",
        "\n",
        "results = model.train(data=f'{dataset.location}/data.yaml', epochs=50)\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=\"wGYGwTjglhST7mybt9qF\")\n",
        "\n",
        "# Load your\n",
        "## Evaluate fine-tuned YOLOv12 model workspace and project\n",
        "project = rf.workspace(\"da-mec7y\").project(\"cataract-finder-nups1\")\n",
        "\n",
        "import locale\n",
        "locale.getpreferredencoding = lambda: \"UTF-8\"\n",
        "\n",
        "from IPython.display import Image, display\n",
        "\n",
        "import supe\n",
        "from supervision.metrics import MeanAveragePrecision\n",
        "\n",
        "model = YOLO(f'/{HOME}/runs/detect/train/weights/best.pt')\n",
        "\n",
        "predictions\n",
        "print(\"mAP 50:95\", map.map50_95)\n",
        "print(\"mAP 50\", map.map50)\n",
        "print(\"mAP 75\", map.map75) = []\n",
        "targets = []\n",
        "\n",
        "map.plot()\n",
        "for _, image, target in ds:\n",
        "    results = model(image, verbose=False)[0]\n",
        "    detections = sv.Detections.from_ultralytics(results)\n",
        "\n",
        "\n",
        "import supervision as sv\n",
        "\n",
        "model = YOLO(f'/{HOME}/runs/detect/train/weights/best.pt')\n",
        "\n",
        "import random\n",
        "\n",
        "i = random.randint(0, len(ds))\n",
        "\n",
        "image_path, image, target = ds[i]\n",
        "\n",
        "results = model(image, verbose=False)[0]\n",
        "detections = sv.Detections.from_ultralytics(results).with_nms()\n",
        "\n",
        "box_annotator = sv.BoxAnnotator()\n",
        "label_annotator = sv.LabelAnnotator()\n",
        "\n",
        "annotated_image = image.copy()\n",
        "annotated_image = box_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "annotated_image = label_annotator.annotate(scene=annotated_image, detections=detections)\n",
        "\n",
        "sv.plot_ima\n",
        "ge(annotated_image)\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{dataset.location}/test/images\",\n",
        "    annotations_directory_path=f\"{dataset.location}/test/labels\",\n",
        "    data_yaml_path=f\"{dataset.location}/data.yaml\"\n",
        ")    predictions.append(detections)\n",
        "    targets.append(target)\n",
        "\n",
        "map = MeanAveragePrecision().update(predictions, targets).compute()rvision as sv\n",
        "\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{dataset.location}/test/images\",\n",
        "    annotations_directory_path=f\"{dataset.location}/test/labels\",\n",
        "    data_yaml_path=f\"{dataset.location}/data.yaml\"\n",
        ")\n",
        "\n",
        "ds.classes\n",
        "import supervision as sv\n",
        "\n",
        "ds = sv.DetectionDataset.from_yolo(\n",
        "    images_directory_path=f\"{dataset.location}/test/images\",\n",
        "    annotations_directory_path=f\"{dataset.location}/test/labels\",\n",
        "    data_yaml_path=f\"{dataset.location}/data.yaml\"\n",
        ")\n",
        "\n",
        "ds.classesimport os\n",
        "\n",
        "HOME = \"/content\"  # Define if not already set\n",
        "\n",
        "image_path = os.path.join(HOME, \"runs/detect/train/confusion_matrix.png\")\n",
        "display(Image(filename=image_path, width=1000))\n",
        "\n",
        "!ls {HOME}/runs/detect/train/\n",
        "# Get the version you want (e.g., version 1) and specify format as 'yolov12'\n",
        "dataset = project.version(1).download(\"yolov12\")\n",
        "\n",
        "from roboflow import Roboflow\n",
        "\n",
        "# Initialize Roboflow\n",
        "rf = Roboflow(api_key=\"wGYGwTjglhST7mybt9qF\")\n",
        "\n",
        "# Load your workspace and project\n",
        "project = rf.workspace(\"da-mec7y\").project(\"cataract-finder-nups1\")\n",
        "\n",
        "# Get the version you want (e.g., version 1) and specify format as 'yolov12'\n",
        "dataset = project.version(1).download(\"yolov12\")\n",
        " dataset from Roboflow Universe\n",
        "## Download dataset from Roboflow Universed example data\n",
        "\n",
        "Let's download an image we can use for YOLOv12 inference. Feel free to drag and drop your own images into the Files tab on the left-hand side of Google Colab, then reference their filenames in your code for a custom inference demo.\n",
        "HOME = os.getcwd()\n",
        "print(HOME)\n",
        "import os\n",
        "HOME = os.getcwd()\n",
        "print(HOME)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
